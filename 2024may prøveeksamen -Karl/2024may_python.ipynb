{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from math import exp\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import ttest_1samp\n",
    "from scipy.stats import mode\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Statistic (t_obs): -1.539600717839002\n",
      "Critical Values (t_critical_lower, t_critical_upper): (-2.011740513729766, 2.0117405137297655)\n"
     ]
    }
   ],
   "source": [
    "# Given values\n",
    "mu = 18  # Null hypothesis mean\n",
    "x_bar = 17  # Sample mean\n",
    "s = 4.5  # Sample standard deviation\n",
    "n = 48  # Sample size\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "# Degrees of freedom\n",
    "df = n - 1\n",
    "\n",
    "# Calculate test statistic (t_obs)\n",
    "t_obs = (x_bar - mu) / (s / math.sqrt(n))\n",
    "\n",
    "# Calculate critical t-values for two-tailed test\n",
    "t_critical_lower = stats.t.ppf(alpha / 2, df)  # Lower critical value\n",
    "t_critical_upper = stats.t.ppf(1 - alpha / 2, df)  # Upper critical value\n",
    "\n",
    "# Output results\n",
    "print(\"Test Statistic (t_obs):\", t_obs)\n",
    "print(\"Critical Values (t_critical_lower, t_critical_upper):\", (t_critical_lower, t_critical_upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0889\n",
      "Fail to reject the null hypothesis (H0).\n"
     ]
    }
   ],
   "source": [
    "t_obs = -1.74\n",
    "n = 45\n",
    "alpha = 0.05\n",
    "df = n - 1\n",
    "p_value = 2 * t.cdf(t_obs, df)\n",
    "print(f\"{p_value:.4f}\")\n",
    "# Decision\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis (H0).\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis (H0).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval for Standard Deviation: (0.072, 0.121)\n"
     ]
    }
   ],
   "source": [
    "# Given values\n",
    "n = 30\n",
    "x_bar = 1.01  # sample mean\n",
    "std = 0.09  # sample standard deviation\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "df = n - 1  # degrees of freedom\n",
    "\n",
    "# Calculate chi-square critical values\n",
    "chi2_lower = stats.chi2.ppf(alpha / 2, df)  # lower critical value\n",
    "chi2_upper = stats.chi2.ppf(1 - alpha / 2, df)  # upper critical value\n",
    "\n",
    "# Confidence interval for standard deviation\n",
    "std_lower = np.sqrt((df * std**2) / chi2_upper)  # lower bound\n",
    "std_upper = np.sqrt((df * std**2) / chi2_lower)  # upper bound\n",
    "\n",
    "# Output the confidence interval\n",
    "print(\"Confidence Interval for Standard Deviation: ({:.3f}, {:.3f})\".format(std_lower, std_upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability P(X < 3) is: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Define the probability distribution\n",
    "values_of_x = [0, 1, 2, 3, 4]  # Possible values of X\n",
    "f_x = [0.17, 0.22, 0.28, 0.0, 0.33]  # Corresponding probabilities for each X\n",
    "# Calculate P(X < 3)\n",
    "# This includes probabilities for X = 0, X = 1, and X = 2\n",
    "probability_x_less_than_3 = sum(f_x[0:3])  # Summing up probabilities for X < 3\n",
    "# Output the result\n",
    "print(f\"The probability P(X < 3) is: {probability_x_less_than_3:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of X: 2.21\n"
     ]
    }
   ],
   "source": [
    "# Given values\n",
    "values_of_x = [0, 1, 2, 3, 4]  # Possible values of X\n",
    "f_x = [0.17, 0.22, 0.28, 0.0, 0.33]  # Corresponding probabilities for each X\n",
    "mean_X = 2.10  # given mean\n",
    "# Calculate variance\n",
    "variance_X = sum([f_x[i] * (values_of_x[i] - mean_X)**2 for i in range(len(values_of_x))])\n",
    "print(\"Variance of X:\", variance_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect: -2.5\n"
     ]
    }
   ],
   "source": [
    "# Effect is how much the location deviates from the mean\n",
    "# Effect = location - mean\n",
    "\n",
    "# Given values\n",
    "mean = 252.5\n",
    "location = 250.0\n",
    "effect = location - mean \n",
    "print(\"Effect:\", effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Standard Deviation: 6.027\n"
     ]
    }
   ],
   "source": [
    "# Given values\n",
    "SST = 915.92  # Total sum of squares\n",
    "SS_location = 480.00  # Sum of squares for location\n",
    "total_observations = 16  # total obs on the table\n",
    "number_of_groups = 4  # 4 locations\n",
    "\n",
    "# Calculate SSE (Residual sum of squares)\n",
    "SSE = SST - SS_location\n",
    "\n",
    "# Calculate degrees of freedom for error\n",
    "df_error = total_observations - number_of_groups\n",
    "\n",
    "# Calculate error standard deviation\n",
    "error_std = np.sqrt(SSE / df_error)\n",
    "\n",
    "# Output the result\n",
    "print(f\"Error Standard Deviation: {error_std:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated β1: 467.6\n"
     ]
    }
   ],
   "source": [
    "# Given data\n",
    "x = np.array([0, 0, 2, 2, 4, 4, 6, 6, 8, 8, 12, 12])  # PFOS concentrations\n",
    "y = np.array([16, 116, 1170, 841, 2287, 2012, 2653, 3333, 4270, 3999, 5750, 5407])  # SPETT values\n",
    "# Step 1: Calculate means of x and y\n",
    "mean_x = np.mean(x)\n",
    "mean_y = np.mean(y)\n",
    "# Step 2: Calculate the numerator and denominator for β1\n",
    "numerator = np.sum((x - mean_x) * (y - mean_y))\n",
    "denominator = np.sum((x - mean_x) ** 2)\n",
    "# Step 3: Calculate the estimate of β1\n",
    "beta_1 = numerator / denominator\n",
    "# Print the result\n",
    "print(f\"Estimated β1: {beta_1:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate of β₀ (Intercept): 160.729\n",
      "Standard Error of β₀: 125.745\n",
      "t-Statistic: 1.278\n",
      "p-Value: 0.2300\n",
      "We do not reject the null hypothesis, since the p-value is 0.2300.\n"
     ]
    }
   ],
   "source": [
    "# Given data\n",
    "x = np.array([0, 0, 2, 2, 4, 4, 6, 6, 8, 8, 12, 12])  # PFOS concentrations\n",
    "y = np.array([16, 116, 1170, 841, 2287, 2012, 2653, 3333, 4270, 3999, 5750, 5407])  # SPETT values\n",
    "\n",
    "# Step 1: Calculate means\n",
    "mean_x = np.mean(x)\n",
    "mean_y = np.mean(y)\n",
    "\n",
    "# Step 2: Calculate beta_1 (slope)\n",
    "numerator = np.sum((x - mean_x) * (y - mean_y))\n",
    "denominator = np.sum((x - mean_x) ** 2)\n",
    "beta_1 = numerator / denominator\n",
    "\n",
    "# Step 3: Calculate beta_0 (intercept)\n",
    "beta_0 = mean_y - beta_1 * mean_x\n",
    "\n",
    "# Step 4: Calculate residuals and standard error of the regression\n",
    "y_pred = beta_0 + beta_1 * x\n",
    "residuals = y - y_pred\n",
    "SSE = np.sum(residuals**2)\n",
    "n = len(x)\n",
    "df = n - 2  # Degrees of freedom = n - 2\n",
    "MSE = SSE / df\n",
    "SE_beta_0 = np.sqrt(MSE * (1 / n + (mean_x**2) / np.sum((x - mean_x)**2)))\n",
    "\n",
    "# Step 5: Calculate t-statistic for beta_0\n",
    "t_stat = beta_0 / SE_beta_0\n",
    "\n",
    "# Step 6: Calculate p-value (two-tailed test)\n",
    "p_value = 2 * (1 - t.cdf(abs(t_stat), df))\n",
    "\n",
    "# Step 7: Decision based on p-value\n",
    "if p_value < 0.05:\n",
    "    decision = f\"We reject the null hypothesis, since the p-value is {p_value:.4f}.\"\n",
    "else:\n",
    "    decision = f\"We do not reject the null hypothesis, since the p-value is {p_value:.4f}.\"\n",
    "\n",
    "# Output results\n",
    "print(f\"Estimate of β₀ (Intercept): {beta_0:.3f}\")\n",
    "print(f\"Standard Error of β₀: {SE_beta_0:.3f}\")\n",
    "print(f\"t-Statistic: {t_stat:.3f}\")\n",
    "print(f\"p-Value: {p_value:.4f}\")\n",
    "print(decision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted SPETT value at x = 7: 3433.80\n",
      "Prediction Interval: [2829, 4039]\n"
     ]
    }
   ],
   "source": [
    "# Given data\n",
    "x = np.array([0, 0, 2, 2, 4, 4, 6, 6, 8, 8, 12, 12])  # PFOS concentrations\n",
    "y = np.array([16, 116, 1170, 841, 2287, 2012, 2653, 3333, 4270, 3999, 5750, 5407])  # SPETT values\n",
    "x_new = 7  # New PFOS concentration\n",
    "\n",
    "# Step 1: Calculate means and regression coefficients\n",
    "mean_x = np.mean(x)\n",
    "mean_y = np.mean(y)\n",
    "beta_1 = np.sum((x - mean_x) * (y - mean_y)) / np.sum((x - mean_x) ** 2)\n",
    "beta_0 = mean_y - beta_1 * mean_x\n",
    "\n",
    "# Step 2: Predict y at x_new\n",
    "y_pred_new = beta_0 + beta_1 * x_new\n",
    "\n",
    "# Step 3: Calculate residuals, MSE, and degrees of freedom\n",
    "y_pred = beta_0 + beta_1 * x\n",
    "residuals = y - y_pred\n",
    "SSE = np.sum(residuals**2)\n",
    "n = len(x)\n",
    "df = n - 2  # Degrees of freedom = n - 2\n",
    "MSE = SSE / df\n",
    "\n",
    "# Step 4: Calculate SE_pred\n",
    "SE_pred = np.sqrt(\n",
    "    MSE * (1 + 1 / n + ((x_new - mean_x) ** 2) / np.sum((x - mean_x) ** 2))\n",
    ")\n",
    "\n",
    "# Step 5: Calculate the critical t-value\n",
    "alpha = 0.05  # 95% confidence level\n",
    "t_critical = t.ppf(1 - alpha / 2, df)\n",
    "\n",
    "# Step 6: Calculate the prediction interval\n",
    "margin_of_error = t_critical * SE_pred\n",
    "lower_bound = y_pred_new - margin_of_error\n",
    "upper_bound = y_pred_new + margin_of_error\n",
    "\n",
    "# Output the result\n",
    "print(f\"Predicted SPETT value at x = {x_new}: {y_pred_new:.2f}\")\n",
    "print(f\"Prediction Interval: [{lower_bound:.0f}, {upper_bound:.0f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Proportion (p̂): 0.235\n",
      "Critical z-value: 1.645\n",
      "Standard Error (SE): 0.046\n",
      "Confidence Interval: [0.160, 0.311]\n"
     ]
    }
   ],
   "source": [
    "# Given data\n",
    "x = 20  # Number of batches not meeting quality standards\n",
    "n = 85  # Total number of batches\n",
    "alpha = 0.10  # Significance level\n",
    "\n",
    "# Step 1: Calculate sample proportion\n",
    "p_hat = x / n\n",
    "\n",
    "# Step 2: Find critical z-value for 90% confidence level (alpha/2 = 0.05)\n",
    "z_critical = norm.ppf(1 - alpha / 2)\n",
    "\n",
    "# Step 3: Calculate standard error\n",
    "SE = np.sqrt(p_hat * (1 - p_hat) / n)\n",
    "\n",
    "# Step 4: Compute confidence interval\n",
    "lower_bound = p_hat - z_critical * SE\n",
    "upper_bound = p_hat + z_critical * SE\n",
    "\n",
    "# Output results\n",
    "print(f\"Sample Proportion (p̂): {p_hat:.3f}\")\n",
    "print(f\"Critical z-value: {z_critical:.3f}\")\n",
    "print(f\"Standard Error (SE): {SE:.3f}\")\n",
    "print(f\"Confidence Interval: [{lower_bound:.3f}, {upper_bound:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum sample size: 277\n"
     ]
    }
   ],
   "source": [
    "# Given data for new check\n",
    "p_hat = 20 / 85  # Observed proportion from previous check\n",
    "alpha = 0.05  # 95% confidence level\n",
    "w = 0.1  # Desired width of the confidence interval\n",
    "\n",
    "# Step 1: Calculate z-critical value for 95% confidence\n",
    "z_critical = norm.ppf(1 - alpha / 2)\n",
    "\n",
    "# Step 2: Calculate minimum sample size\n",
    "n_required = (z_critical / (w / 2))**2 * p_hat * (1 - p_hat)\n",
    "\n",
    "# Output (rounding up to the nearest integer)\n",
    "n_required = int(np.ceil(n_required))\n",
    "print(f\"Minimum sample size: {n_required}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: -1.488\n",
      "P-value: 0.197\n",
      "Fail to reject the null hypothesis: No significant improvement in scores.\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "pre = np.array([41, 46, 35, 49, 33, 42])  # Pre-test scores\n",
    "post = np.array([42, 47, 43, 55, 28, 49])  # Post-test scores\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_stat, p_value = ttest_rel(pre, post)\n",
    "\n",
    "# Output the results\n",
    "print(f\"T-statistic: {t_stat:.3f}\")\n",
    "print(f\"P-value: {p_value:.3f}\")\n",
    "\n",
    "# Decision\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant improvement in scores.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant improvement in scores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.109\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 2.5  # Average number of forest fires per day\n",
    "k = 5  # At least 5 fires\n",
    "\n",
    "# Calculate probability for X >= 5\n",
    "# P(X >= 5) = 1 - P(X <= 4)\n",
    "probability_at_least_5 = 1 - poisson.cdf(k - 1, lambda_)\n",
    "\n",
    "# Output the result\n",
    "print(f\"{probability_at_least_5:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total mean for 7 days: 17.5\n",
      "The distribution is: Poisson with mean = 17.5\n"
     ]
    }
   ],
   "source": [
    "# Given parameters\n",
    "lambda_per_day = 2.5  # Average forest fires per day\n",
    "days = 7  # Number of days\n",
    "# Step 1: Calculate the total mean (lambda) for 7 days\n",
    "lambda_total = lambda_per_day * days\n",
    "# Step 2: Verify the distribution\n",
    "# The distribution of the sum of Poisson random variables is also Poisson\n",
    "print(f\"The total mean for 7 days: {lambda_total}\")\n",
    "print(\"The distribution is: Poisson with mean =\", lambda_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Expected Events Over 7 Days: 17.5\n"
     ]
    }
   ],
   "source": [
    "# Given parameters\n",
    "p = 0.78  # Probability of a single forest fire being avoidable\n",
    "n = 5  # Total number of forest fires\n",
    "# Probability that all 5 fires could have been avoided\n",
    "prob_all_avoidable = p**n\n",
    "prob_all_avoidable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28871743680000006"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = 4.03  # Sample mean of wing lengths\n",
    "std_dev = 0.49  # Sample standard deviation of wing lengths\n",
    "n = 30  # Sample size\n",
    "n_iterations = 10000  # Number of bootstrap iterations\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(2023)\n",
    "\n",
    "# Perform parametric bootstrapping\n",
    "bootstrap_medians = [\n",
    "    np.median(np.random.normal(loc=mean, scale=std_dev, size=n))\n",
    "    for _ in range(n_iterations)\n",
    "]\n",
    "\n",
    "# Calculate 99% confidence interval\n",
    "ci_lower = np.percentile(bootstrap_medians, 0.5)  # Lower bound of 99% CI\n",
    "ci_upper = np.percentile(bootstrap_medians, 99.5)  # Upper bound of 99% CI\n",
    "\n",
    "(ci_lower, ci_upper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.742269006911915, 4.31233884462923)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given data\n",
    "n1 = 12 # Sample size for Group 1\n",
    "n2 = 12 # Sample size for Group 2\n",
    "x1_bar = 325 # Sample mean for Group 1\n",
    "x2_bar = 286 # Sample mean for Group 2\n",
    "s1 = 40 # Sample standard deviation for Group 1\n",
    "s2 = 44 # Sample standard deviation for Group 2\n",
    "\n",
    "# Calculate the t-test statistic (t_obs)\n",
    "t_obs = (x1_bar - x2_bar) / np.sqrt((s1 / n1) + (s2 / n2))\n",
    "\n",
    "# Calculate the degrees of freedom (df)\n",
    "numerator = ((s1 / n1) + (s2 / n2))**2\n",
    "denominator = ((s1 / n1)**2 / (n1 - 1)) + ((s2 / n2)**2 / (n2 - 1))\n",
    "df = numerator / denominator\n",
    "\n",
    "# Output the results\n",
    "print(f\"t-Statistic (t_obs): {t_obs:.4f}\")\n",
    "print(f\"Degrees of Freedom (df): {df:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic (t_obs): 2.27\n",
      "Degrees of freedom (df): 21.8\n"
     ]
    }
   ],
   "source": [
    "# Given values\n",
    "x1_mean = 325  # Sample mean for current technology\n",
    "x2_mean = 286  # Sample mean for new software package\n",
    "s1 = 40  # Sample standard deviation for current technology\n",
    "s2 = 44  # Sample standard deviation for new software package\n",
    "n1 = 12  # Sample size for current technology\n",
    "n2 = 12  # Sample size for new software package\n",
    "# Calculate t_obs (t-statistic)\n",
    "t_obs = (x1_mean - x2_mean) / np.sqrt((s1**2 / n1) + (s2**2 / n2))\n",
    "# Calculate degrees of freedom using Welch's formula\n",
    "df_numerator = (s1**2 / n1 + s2**2 / n2)**2\n",
    "df_denominator = ((s1**2 / n1)**2 / (n1 - 1)) + ((s2**2 / n2)**2 / (n2 - 1))\n",
    "df = df_numerator / df_denominator\n",
    "# Output the results\n",
    "print(f\"t-statistic (t_obs): {t_obs:.2f}\")\n",
    "print(f\"Degrees of freedom (df): {df:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5801633505712303, 0.5753641449035618)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of realizations\n",
    "k = 10000  # Large enough to approximate the distribution\n",
    "\n",
    "# Simulate X_i as rnorm(k)^2 + rnorm(k)^2\n",
    "x = np.random.normal(0, 1, k)**2 + np.random.normal(0, 1, k)**2\n",
    "\n",
    "# Identify the distribution\n",
    "# Test for Chi-squared distribution with 2 degrees of freedom (sum of squares of 2 independent normal RVs)\n",
    "# Compare the 25th percentile (q0.25)\n",
    "q0_25 = np.percentile(x, 25)  # Empirical 25th percentile\n",
    "\n",
    "# Theoretical 25th percentile for chi-squared with 2 degrees of freedom\n",
    "theoretical_q0_25 = stats.chi2.ppf(0.25, df=2)\n",
    "\n",
    "(q0_25, theoretical_q0_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Non-parametric bootstrapping is a re-sampling technique used to estimate the variability \n",
    "#    of a parameter without making any assumptions about the underlying population.\n",
    "#    Answer: TRUE. Non-parametric bootstrapping does not assume any specific population distribution.\n",
    "\n",
    "# 2. Non-parametric bootstrapping involves repeatedly sampling with replacement from the\n",
    "#    original sample to create many new, simulated samples of the same size as the original sample.\n",
    "#    Answer: TRUE. This is how non-parametric bootstrapping works, by sampling with replacement.\n",
    "\n",
    "# 3. Non-parametric bootstrapping should be preferred over parametric bootstrapping if you \n",
    "#    know the distribution of the population.\n",
    "#    Answer: FALSE. If the population distribution is known, parametric bootstrapping is preferred \n",
    "#    because it uses the known distribution for simulation, which is more efficient.\n",
    "\n",
    "# 4. Non-parametric bootstrapping can be applied to estimate the 95% confidence interval of a sample mean.\n",
    "#    Answer: TRUE. Non-parametric bootstrapping is commonly used to construct confidence intervals \n",
    "#    for the sample mean or other statistics.\n",
    "\n",
    "# 5. Non-parametric bootstrapping can be used to estimate confidence intervals for a parameter \n",
    "#    and to test hypotheses.\n",
    "#    Answer: TRUE. Non-parametric bootstrapping is versatile and can estimate confidence intervals \n",
    "#    and test hypotheses based on the resampled statistics.\n",
    "\n",
    "# The correct answer is: 3. It is FALSE because parametric bootstrapping is better if the population distribution is known.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. \"The effect of catalyst on efficiency is not significant, because the p-value is less than 0.01.\"\n",
    "#    Incorrect. The p-value for the catalyst is 0.00184, which is less than 0.01, making it significant.\n",
    "\n",
    "# 2. \"The amount of catalyst has a significant effect on efficiency, while the pH does not.\"\n",
    "#    Correct. Catalyst is significant (p = 0.00184), while pH is not (p = 0.01687) at α = 0.01.\n",
    "\n",
    "# 3. \"Both the amount of catalyst and the pH are significant, because the p-values are less than 0.05.\"\n",
    "#    Incorrect. While both p-values are less than 0.05, the question specifically asks for α = 0.01. \n",
    "#    At this level, only the catalyst is significant.\n",
    "\n",
    "# 4. \"Neither the amount of catalyst nor the pH are significant, because the p-values are less than 0.05.\"\n",
    "#    Incorrect. This statement is contradictory. Both are significant at α = 0.05, and the catalyst is significant \n",
    "#    even at α = 0.01.\n",
    "\n",
    "# 5. \"The model intercept is significant, because the p-value of 0.0150 is greater than 0.01.\"\n",
    "#    Incorrect. The intercept's p-value is 0.01496, which is greater than 0.01, so it is not significant at α = 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.186"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients from the R output\n",
    "catalyst_coefficient = 4.593  # Coefficient for catalyst from the R output\n",
    "catalyst_increase = 2  # Increase in catalyst (2 units)\n",
    "# Calculate the effect on expected hourly output\n",
    "expected_increase = catalyst_coefficient * catalyst_increase\n",
    "expected_increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.377510040160644"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observed data\n",
    "trace = np.array([12, 14, 18, 5, 15])  # Plants with trace\n",
    "no_trace = np.array([38, 35, 35, 42, 35])  # Plants with no trace\n",
    "\n",
    "# Total for each chemical\n",
    "total_per_chemical = trace + no_trace\n",
    "\n",
    "# Total plants with no trace across all chemicals\n",
    "total_no_trace = np.sum(no_trace)\n",
    "\n",
    "# Total plants overall\n",
    "total_plants = np.sum(trace) + np.sum(no_trace)\n",
    "\n",
    "# Expected number of plants with no trace for chemical C\n",
    "expected_no_trace_C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.24929193000206,\n",
       " 0.08286151800665145,\n",
       " 4,\n",
       " array([[12.85140562, 12.59437751, 13.62248996, 12.08032129, 12.85140562],\n",
       "        [37.14859438, 36.40562249, 39.37751004, 34.91967871, 37.14859438]]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observed data as a contingency table\n",
    "data = np.array([\n",
    "    [12, 14, 18, 5, 15],  # Plants with trace\n",
    "    [38, 35, 35, 42, 35]  # Plants with no trace\n",
    "])\n",
    "\n",
    "# Perform Chi-squared test\n",
    "chi2_stat, p_value, dof, expected = chi2_contingency(data)\n",
    "\n",
    "# Output the results\n",
    "chi2_stat, p_value, dof, expected\n",
    "\n",
    "# All exppected counts are above 5, so the chi-squared test is valid. (checked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.575734721155486,\n",
       " 0.048214114523735675,\n",
       " 4,\n",
       " array([[20.14 , 16.695, 16.165],\n",
       "        [25.46 , 21.105, 20.435],\n",
       "        [30.4  , 25.2  , 24.4  ]]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observed data as a contingency table\n",
    "data = np.array([\n",
    "    [25, 9, 19],  # Low quality (row 1)\n",
    "    [23, 21, 23],  # Medium quality (row 2)\n",
    "    [28, 33, 19]   # High quality (row 3)\n",
    "])\n",
    "# Perform Chi-squared test\n",
    "chi2_stat, p_value, dof, expected = chi2_contingency(data)\n",
    "# Output the results\n",
    "chi2_stat, p_value, dof, expected\n",
    "\n",
    "# not independent. as p-value is less than 0.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.5"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([77, 111, 71, 91, 116, 81, 83, 80, 86, 92])\n",
    "median = np.median(x)\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Variance: 209.29\n"
     ]
    }
   ],
   "source": [
    "x = np.array([77, 111, 71, 91, 116, 81, 83, 80, 86, 92])\n",
    "sample_variance = np.var(x, ddof=1)  # ddof=1 for sample variance\n",
    "print(f\"Sample Variance: {sample_variance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a is the historgram\n",
    "# histogram is normaly lower bound 80-89, 90-99, 100-109, 110-119, 120-129\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
